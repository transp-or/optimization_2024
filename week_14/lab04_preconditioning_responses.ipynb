{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a022001",
   "metadata": {},
   "source": [
    "# Non linear optimization: preconditioning\n",
    "\n",
    "## Introduction to optimization and operations research\n",
    "\n",
    "Michel Bierlaire\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23cbc637",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21077b96",
   "metadata": {},
   "source": [
    "Consider the function $f:\\mathbb{R}^2 \\to \\mathbb{R}$ defined as\n",
    "$$\n",
    "f(x)= \\frac{1}{2}x_1^2 + \\frac{101}{2} x_2^2 +  x_1 x_2.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33471f85",
   "metadata": {},
   "source": [
    "# Question 1\n",
    "Implement a function in Python that calculates the function and its first and second derivatives."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432fc855",
   "metadata": {},
   "source": [
    "The derivatives of the function are\n",
    "$$\n",
    "\\nabla f(x) = \\begin{pmatrix*} x_1+x_2\n",
    "\\\\ x_1+101x_2 \\end{pmatrix*},\\;\n",
    "\\nabla^2 f(x) =\n",
    "\\begin{pmatrix*}\n",
    "1 & 1 \\\\  1 & 101\n",
    "\\end{pmatrix*}.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f597b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def the_function(x: np.array) -> tuple[float, np.array, np.array]:\n",
    "    \"\"\"Calculates the function and its derivatives\n",
    "\n",
    "    :param x: a vector of dimension 2\n",
    "    :return: a tuple with the value of the function, the gradient and the second derivatives matrix\n",
    "    \"\"\"\n",
    "    f = 0.5 * x[0] * x[0] + 101 / 2 * x[1] * x[1] + x[0] * x[1]\n",
    "    g = np.array([x[0] + x[1], x[0] + 101 * x[1]])\n",
    "    h = np.array([[1, 1], [1, 101]])\n",
    "    return f, g, h\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434d9b26",
   "metadata": {},
   "source": [
    "Test it at the point $(1, 1)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695cf6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([1, 1])\n",
    "function, gradient, hessian = the_function(x)\n",
    "print(f'f(x)={function}')\n",
    "print(f'gradient(x)={gradient}')\n",
    "print(f'hessian(x)=\\n{hessian}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f42d760",
   "metadata": {},
   "source": [
    "# Question 2\n",
    "Consider a change of variables\n",
    "$$\n",
    "x' = L_k^T x,\n",
    "$$\n",
    "Consider the function in the new variables\n",
    "$$\n",
    "\\tilde{f}(x') = f(L_k^{-T} x').\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34e8c9f",
   "metadata": {},
   "source": [
    "The gradient of the new function is\n",
    "$$ \\nabla \\tilde{f}(x') = L_k^{-1} \\nabla f(L_k^{-T} x'),$$ that is, the solution of the system:\n",
    "$$L_k \\nabla \\tilde{f}(x') = \\nabla f(L_k^{-T} x').$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8064464",
   "metadata": {},
   "source": [
    "The hessian of the new function is\n",
    "$$ \\nabla^2 \\tilde{f}(x') = L_k^{-1} \\nabla^2 f(L_k^{-T} x') L_k^{-T}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84500ce",
   "metadata": {},
   "source": [
    "that is, the solution of the system:\n",
    "$$L_k \\nabla^2 \\tilde{f}(x') =  D^T_k,$$\n",
    "where $D_k$ is the solution of the system\n",
    "$$\\nabla^2 f(L_k^{-T} x') = L_k D_k$$\n",
    "Implement a Python function that calculates this function and its first and second derivatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add87341",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def preconditioned_function(\n",
    "    x: np.array, l_k: np.array\n",
    ") -> tuple[float, np.array, np.array]:\n",
    "    \"\"\"Calculates the preconditioned function and its gradient.\n",
    "\n",
    "    :param x: a vector of dimension 2.\n",
    "    :param l_k:  matrix defining the change of variables.\n",
    "    :return: a tuple with the value of the function, the gradient and the hessian.\n",
    "    \"\"\"\n",
    "    x_original = np.linalg.solve(l_k.T, x)\n",
    "    f, g, h = the_function(x_original)\n",
    "    the_gradient = np.linalg.solve(l_k, g)\n",
    "    d_k = np.linalg.solve(l_k, h)\n",
    "    the_hessian = np.linalg.solve(l_k, d_k.T)\n",
    "    return f, the_gradient, the_hessian\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0f8afe",
   "metadata": {},
   "source": [
    "Consider $L_k$ to be the Cholesky factor of the second derivative matrix\n",
    "$$\n",
    "L_k L_k^T=  \\nabla^2 f(x_k).\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17a112a",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_k = np.linalg.cholesky(hessian)\n",
    "print(l_k)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560cfec6",
   "metadata": {},
   "source": [
    "Check that it is indeed the Cholesky factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b0934e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(l_k @ l_k.T)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686888e7",
   "metadata": {},
   "source": [
    "It must be the same as the hessian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fac7d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(hessian)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ae5d61",
   "metadata": {},
   "source": [
    "Evaluate the preconditioned function at the point $x'=L_k^T x$, where $x=(1,1)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62fd25df",
   "metadata": {},
   "outputs": [],
   "source": [
    "prec_x = l_k.T @ x\n",
    "print(f'{prec_x=}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38477b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "prec_function, prec_gradient, prec_hessian = preconditioned_function(prec_x, l_k)\n",
    "print(f'Preconditioned f(x)={prec_function}')\n",
    "print(f'Preconditioned gradient(x)={prec_gradient}')\n",
    "print(f'Preconditioned hessian(x)=\\n{prec_hessian}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b07714",
   "metadata": {},
   "source": [
    "\n",
    "Note that the second derivatives matrix is the identify matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2177fa1e",
   "metadata": {},
   "source": [
    "Here are some additional details to understand the above results.\n",
    "The matrix $L_k$ defining the change of variables is the\n",
    "Cholesky factor of the second derivatives matrix, that is\n",
    "$$\n",
    "L_k = \\begin{pmatrix*}\n",
    "1 & 0 \\\\ 1 & 10\n",
    "\\end{pmatrix*}, \\; \\forall k,\n",
    "$$\n",
    "as\n",
    "$$\n",
    "L_k L_k^T = \\begin{pmatrix*}\n",
    "1 & 1 \\\\  1 & 101\n",
    "\\end{pmatrix*}.\n",
    "$$\n",
    "Therefore, the change of variables is $x'=L_k^Tx$, that is\n",
    "$$\n",
    "x' = \\begin{pmatrix*}\n",
    "1 & 1 \\\\ 0 & 10\n",
    "\\end{pmatrix*} x\n",
    "$$\n",
    "or\n",
    "\\begin{align*}\n",
    "x_1' &= x_1 + x_2 \\\\\n",
    "x_2' &= 10 x_2.\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e514695",
   "metadata": {},
   "source": [
    "We write the change of variables in the opposite direction as\n",
    "$x=L_k^{-T} x'$, that is\n",
    "$$\n",
    "x = \\left(\\begin{array}{rr}\n",
    "1 & -\\frac{1}{10} \\\\ 0 & \\frac{1}{10}\n",
    "\\end{array}\\right) x'\n",
    "$$\n",
    "or\n",
    "\\begin{align*}\n",
    "x_1 &= x_1' -\\frac{1}{10} x_2', \\\\\n",
    "x_2 &= \\frac{1}{10} x_2'.\n",
    "\\end{align*}\n",
    "The function $\\tilde{f}$ is therefore defined as\n",
    "\\begin{align*}\n",
    "\\tilde{f}(x') &= f(x_1' -\\frac{1}{10} x_2',\\frac{1}{10} x_2') \\\\\n",
    "&=  \\frac{1}{2}\\left(x_1' -\\frac{1}{10} x_2'\\right)^2 +\n",
    "\\frac{101}{2} \\left(\\frac{1}{10} x_2'\\right)^2 +  \\left(x_1'\n",
    "-\\frac{1}{10} x_2'\\right) \\left(\\frac{1}{10} x_2'\\right), \\\\\n",
    "&= \\frac{1}{2} x_1'^2 + \\frac{1}{2} x_2'^2.\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97315d1d",
   "metadata": {},
   "source": [
    "The derivatives of $\\tilde{f}$ are\n",
    "$$\n",
    "\\nabla \\tilde{f}(x'_1,x'_2) = \\left(\\begin{array}{c}x'_1 \\\\ x'_2\\end{array}\\right),\\;\n",
    "\\nabla^2 \\tilde{f} =\n",
    "\\begin{pmatrix*}\n",
    "1 & 0 \\\\ 0 & 1\n",
    "\\end{pmatrix*}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295820c4",
   "metadata": {},
   "source": [
    "# Question 3\n",
    "Apply one iteration of the steepest descent\n",
    "algorithm on $\\tilde{f}$ from that point, that is\n",
    "$$\n",
    "x'_{k+1} = x'_k - \\alpha \\nabla \\tilde{f}(x'_k),\n",
    "$$\n",
    "where the step size is\n",
    "$$\n",
    "\\alpha = \\frac{\\nabla \\tilde{f}(x'_k)^T \\nabla \\tilde{f}(x'_k)}{\\nabla\n",
    "\\tilde{f}(x'_k)^T \\nabla^2 \\tilde{f}(x'_k) \\nabla \\tilde{f}(x'_k)}.\n",
    "$$\n",
    "It is the Cauchy point."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9689b0d9",
   "metadata": {},
   "source": [
    "\n",
    "The point in the new variables:\n",
    "$$\n",
    "x_0 = \\left(\\begin{array}{c}1 \\\\ 1\\end{array}\\right), \\; {x_0}' = \\left(\\begin{array}{c}2 \\\\ 10\\end{array}\\right),\n",
    "$$\n",
    "so that\n",
    "$$\n",
    "\\nabla \\tilde{f}({x_0}') = \\left(\\begin{array}{c}2 \\\\ 10\\end{array}\\right).\n",
    "$$\n",
    "The step to perform along the steepest descent direction is\n",
    "$$\n",
    "\\alpha = \\frac{\\nabla \\tilde{f}({x_0}')^T\\nabla\n",
    "\\tilde{f}({x_0}')}{\\nabla \\tilde{f}({x_0}')^T\\nabla^2\n",
    "\\tilde{f}({x_0}')\\nabla \\tilde{f}({x_0}')}.\n",
    "$$\n",
    "As $\\nabla^2 \\tilde{f}({x_0}')$ is the identity matrix,\n",
    "$\\alpha=1$. Actually, this would be true for any $x_0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b6d165",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = np.dot(prec_gradient, prec_gradient) / np.dot(\n",
    "    prec_gradient, prec_hessian @ prec_gradient\n",
    ")\n",
    "print(f'{alpha=:.3g}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10157d48",
   "metadata": {},
   "source": [
    "Therefore, we obtain\n",
    "$$\n",
    "x'_1 = x'_0 - \\alpha \\nabla \\tilde{f}({x_0}') = \\left(\\begin{array}{c}2 \\\\ 10\\end{array}\\right)\n",
    "- \\left(\\begin{array}{c}2 \\\\ 10\\end{array}\\right) = \\left(\\begin{array}{c}0 \\\\ 0\\end{array}\\right).\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04dc9fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_prec_x = prec_x - alpha * prec_gradient\n",
    "print(f'{new_prec_x=}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3593a920",
   "metadata": {},
   "source": [
    "Identify the corresponding point in the original variables.\n",
    "In the original variables, we have\n",
    "\\begin{align*}\n",
    "x_1 &= x_1' -\\frac{1}{10} x_2' = 0, \\\\\n",
    "x_2 &= \\frac{1}{10} x_2' = 0.\n",
    "\\end{align*}\n",
    "It happens to be the optimal solution of the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92670ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_x = np.linalg.solve(l_k.T, new_prec_x)\n",
    "print(f'{new_x=}')"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
