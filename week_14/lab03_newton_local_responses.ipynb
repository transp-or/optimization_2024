{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "281a0071",
   "metadata": {},
   "source": [
    "# Non linear optimization: Newton local\n",
    "\n",
    "## Introduction to optimization and operations research\n",
    "\n",
    "Michel Bierlaire\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5829defc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from collections.abc import Callable\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d60f3b2",
   "metadata": {},
   "source": [
    "Consider the function $f:\\mathbb{R}^2 \\to \\mathbb{R}$ defined as\n",
    "$$\n",
    "f(x)=\\frac{9}{2} x_1^2 + \\frac{1}{2}x_2^2.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81d7214",
   "metadata": {},
   "source": [
    "The derivatives are:\n",
    "$$\n",
    "\\nabla f(x)=\\left(\\begin{array}{c}9 x_1 \\\\ x_2 \\end{array}\\right),\\;\n",
    "\\nabla^2 f(x) =\n",
    "\\begin{pmatrix*}\n",
    "9 & 0 \\\\ 0 & 1\n",
    "\\end{pmatrix*}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92084328",
   "metadata": {},
   "source": [
    "#  Question 1\n",
    "Implement a Python function that calculates the function above, as well as its gradient and second derivatives\n",
    "matrix. Test it with $x=(1,1)^T$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98c464c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_function(x: np.array) -> tuple[float, np.array, np.array]:\n",
    "    \"\"\"Calculates the objective function and its derivatives\n",
    "\n",
    "    :param x: an array of dimension 2\n",
    "    :return: a tuple with the value of the function, the gradient and the second derivatives matrix\n",
    "    \"\"\"\n",
    "    if len(x) != 2:\n",
    "        error_msg = f'A vector of size 2 is requested. Not {len(x)}'\n",
    "        raise ValueError(error_msg)\n",
    "\n",
    "    f = (9.0 / 2.0) * x[0] * x[0]\n",
    "    g = np.array([9 * x[0], x[1]])\n",
    "    h = np.array([[9, 0], [0, 1]])\n",
    "    return f, g, h\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a552eaff",
   "metadata": {},
   "source": [
    "Test the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2204c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([1, 1])\n",
    "function, gradient, hessian = objective_function(x)\n",
    "print(f'f(x)={function}')\n",
    "print(f'gradient(x)={gradient}')\n",
    "print(f'hessian(x)={hessian}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c3141f",
   "metadata": {},
   "source": [
    "We implement a function that plots a function with two variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb6e545",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(a_function: Callable[[np.array], float]) -> None:\n",
    "    \"\"\"\n",
    "    Plot a function in 3D.\n",
    "\n",
    "    :param a_function: the function to plot\n",
    "    \"\"\"\n",
    "    x1_values = np.linspace(-10, 10, 100)\n",
    "    x2_values = np.linspace(-10, 10, 100)\n",
    "    X1, X2 = np.meshgrid(x1_values, x2_values)\n",
    "    Y = np.array(\n",
    "        [\n",
    "            [a_function(np.array([x1, x2])) for x1, x2 in zip(row_x1, row_x2)]\n",
    "            for row_x1, row_x2 in zip(X1, X2)\n",
    "        ]\n",
    "    )\n",
    "    fig = plt.figure(figsize=(10, 7))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    ax.plot_surface(X1, X2, Y, cmap='viridis')\n",
    "\n",
    "    ax.set_xlabel('x_1')\n",
    "    ax.set_ylabel('x_2')\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1070d5c",
   "metadata": {},
   "source": [
    "In order to plot, we need a function that returns only the value of the function, not the derivatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8ce18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_to_plot(x: np.array) -> float:\n",
    "    \"\"\"Function to be plotted. We ignore the derivatives\n",
    "\n",
    "    :param x: an array of dimension 2\n",
    "    :return: f(x)\n",
    "    \"\"\"\n",
    "    value, _, _ = objective_function(x)\n",
    "    return value\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4484b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(function_to_plot)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc50dbba",
   "metadata": {},
   "source": [
    "# Question 2\n",
    "Implement a Python function that calculates the quadratic model of the function at a point `x_plus`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25435264",
   "metadata": {},
   "source": [
    "The quadratic model of the function at $x^+$ is\n",
    "\\begin{align*}\n",
    "m_{x^+} (x) &= f(x^+) + (x-x^+)^T \\nabla f(x^+) + \\frac{1}{2}\n",
    "(x-x^+)^T \\nabla^2 f(x^+) (x-x^+), \\\\\n",
    "&= 5 + 9 (x_1-1) +  (x_2 -1) + \\frac{1}{2}(x_1-1)(9x_1 -9)+\n",
    "\\frac{1}{2} (x_2-1)(x_2-1) \\\\\n",
    "&= \\frac{9}{2} x_1^2 + \\frac{1}{2} x_2^2.\n",
    "\\end{align*}\n",
    "Actually, the objective function being already quadratic, the\n",
    "quadratic model is the function itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa5d785",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quadratic_model(x: np.array, x_plus: np.array) -> float:\n",
    "    \"\"\"Calculates the quadratic model around a point.\n",
    "\n",
    "    :param x: an array of dimension 2\n",
    "    :param x_plus: an array of dimension 2 with the point around which the model is built.\n",
    "    :return: the value of the quadratic model.\n",
    "    \"\"\"\n",
    "    the_function, the_gradient, the_hessian = objective_function(x_plus)\n",
    "    the_model = (\n",
    "        the_function\n",
    "        + np.dot(x - x_plus, the_gradient)\n",
    "        + 0.5 * np.dot(x - x_plus, the_hessian @ (x - x_plus))\n",
    "    )\n",
    "    return the_model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391b84fb",
   "metadata": {},
   "source": [
    "We plot the model around the point $(1,1)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a9f804",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_to_plot(x: np.array) -> float:\n",
    "    \"\"\"Function to be plotted.\n",
    "\n",
    "    :param x: an array of dimension 2\n",
    "    :return: f(x)\n",
    "    \"\"\"\n",
    "    x_plus = np.array([1, 1])\n",
    "    value = quadratic_model(x, x_plus)\n",
    "    return value\n",
    "\n",
    "\n",
    "plot(model_to_plot)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0bcc24",
   "metadata": {},
   "source": [
    "# Question 3\n",
    "Calculates Newton's point at $(1, 1)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96740b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([1, 1])\n",
    "function, gradient, hessian = objective_function(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954cf246",
   "metadata": {},
   "source": [
    "We solve the system of equations (Definition 10.3, Eq. (10.14) p. 243)\n",
    "\\begin{align*}\n",
    "\\nabla^2 f(x^+) d &= -\\nabla f(x^+), \\\\\n",
    "\\left(\\begin{array}{c}9 d_1 \\\\d_2\\end{array}\\right) &= -\\left(\\begin{array}{c}9 \\\\ 1\\end{array}\\right).\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b8d3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "direction = np.linalg.solve(hessian, -gradient)\n",
    "print(direction)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5a84d2",
   "metadata": {},
   "source": [
    "The solution is $d_1=-1$, $d_2=-1$. Therefore, Newton's point is\n",
    "$$\n",
    "x_N = \\left(\\begin{array}{c}1\\\\1\\end{array}\\right) + \\left(\\begin{array}{c}-1 \\\\-1\\end{array}\\right) =\n",
    "\\left(\\begin{array}{c}0\\\\0\\end{array}\\right).\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2d21a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "newton_point = x + direction\n",
    "print(newton_point)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7a073f",
   "metadata": {},
   "source": [
    "# Question 4\n",
    "Calculates Cauchy's point at $(1, 1)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24f0e44",
   "metadata": {},
   "source": [
    "We need to consider the quadratic model in the direction of\n",
    "the gradient, that is at a point\n",
    "$$\n",
    "x_\\alpha= \\left(\\begin{array}{c}1-9\\alpha \\\\ 1-\\alpha\\end{array}\\right).\n",
    "$$\n",
    "We have\n",
    "\\begin{align*}\n",
    "m_{x^+} (x_\\alpha) &= \\frac{9}{2}(1-9\\alpha)^2 + \\frac{1}{2} (1-\\alpha)^2, \\\\\n",
    "&= 365 \\alpha^2 - 82 \\alpha + 5.\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db06e62",
   "metadata": {},
   "source": [
    "As $f$ is convex, the minimum can be obtained directly from\n",
    "Eq. (10.17) on page 244."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f4e801",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_cauchy = np.dot(gradient, gradient) / np.dot(gradient, hessian @ gradient)\n",
    "print(f\"Step to obtain Cauchy's point: {alpha_cauchy:.4g}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ef331b",
   "metadata": {},
   "source": [
    "And we calculate Cauchy's point along the steepest descent direction.\n",
    "Using Definition 10.4, Eq.(10.15), p. 243:\n",
    "$$\n",
    "x_C = \\left(\\begin{array}{c}1 \\\\ 1\\end{array}\\right)\n",
    "- \\frac{41}{365} \\left(\\begin{array}{c}9\\\\1\\end{array}\\right) =\n",
    "\\left(\\begin{array}{c}-0.01096\n",
    "\\\\ 0.8877\\end{array}\\right).\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb8aeac",
   "metadata": {},
   "outputs": [],
   "source": [
    "cauchy_point = x - alpha_cauchy * gradient\n",
    "print(cauchy_point)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7caad1c2",
   "metadata": {},
   "source": [
    "# Question 5\n",
    "Perform one iteration of Newton's local method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655966be",
   "metadata": {},
   "source": [
    "An iteration of Newton's local method calculates Newton's\n",
    "point. We have already calculated it:\n",
    "$$\n",
    "x_N = \\left(\\begin{array}{c}0\\\\0\\end{array}\\right).\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11fa7a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_iterate = newton_point\n",
    "print(new_iterate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e62f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, gradient, hessian = objective_function(new_iterate)\n",
    "print(f'{f=}')\n",
    "print(f'{gradient=}')\n",
    "print(f'{hessian=}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffdbc784",
   "metadata": {},
   "source": [
    "We note that the gradient is zero at that point. As the second\n",
    "derivative matrix is positive definite, we have found a minimum of\n",
    "the problem (it is actually the unique minimum). It illustrates an\n",
    "interesting property of Newton's local method: if the objective function is a\n",
    "convex quadratic function, Newton's local method converges in only\n",
    "one iteration."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
