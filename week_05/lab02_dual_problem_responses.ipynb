{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47bf5597",
   "metadata": {},
   "source": [
    "# Dual problem\n",
    "\n",
    "## Introduction to optimization and operations research\n",
    "\n",
    "Michel Bierlaire\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e0754a",
   "metadata": {},
   "source": [
    "This exercise does not require coding in Python."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4793985b",
   "metadata": {},
   "source": [
    "Consider the optimization problem\n",
    "$$\\min_{x \\in \\mathbb{R}^2} x_1^2 + x_2^2$$\n",
    "subject to\n",
    "$$x_1=1.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0d62cc",
   "metadata": {},
   "source": [
    "- What is the optimal solution of this problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6300691f",
   "metadata": {},
   "source": [
    "The optimal solution of the problem is $x^*=(1, 0)$, and its\n",
    "optimal value is 1. Indeed, $x_1$ is constrained to be 1. And\n",
    "$x_2$ achieves its minimum at $x_2=0$, irrespectively of the value\n",
    "of $x_1$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9623ff27",
   "metadata": {},
   "source": [
    "- Write the Lagrangian of this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507c7aba",
   "metadata": {},
   "source": [
    "In order to write the Lagrangian, we first express the problem as $$\\min x_1^2+x_2^2$$ subject to\n",
    "$$h_1(x) = x_1 - 1 = 0. \\quad (\\lambda)$$\n",
    "\n",
    "The Lagrangian function of this problem is\n",
    "\\begin{align*}\n",
    "L(x_1,x_2,\\lambda)& = x_1^2+x_2^2 + \\lambda(x_1-1)\\\\\n",
    "& = x_1^2+x_2^2 + \\lambda x_1 - \\lambda.\n",
    "\\end{align*}\n",
    "If is a quadratic function, such that the first derivatives are\n",
    "$$\n",
    "\\frac{\\partial L(x_1,x_2,\\lambda)}{\\partial x_1} = 2 x_1 + \\lambda, \\;\n",
    "\\frac{\\partial L(x_1,x_2,\\lambda)}{\\partial x_2} = 2 x_2,\n",
    "$$\n",
    "and the second derivatives matrix is\n",
    "$$\n",
    "\\nabla^2 f(x) = \\begin{pmatrix}\n",
    "2 & 0 \\\\ 0 & 2\n",
    "\\end{pmatrix}.\n",
    "$$\n",
    "As the second derivatives matrix is positive definite, the quadratic\n",
    "function is convex and, therefore, bounded for any value of\n",
    "$\\lambda$. Consequently, there is no need to impose constraints on the\n",
    "dual variables in this context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6faf86c",
   "metadata": {},
   "source": [
    "- Write the dual function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d8dfef",
   "metadata": {},
   "source": [
    "We are looking for $$q(\\lambda)=\\min_{x \\in\\mathbb{R}^2} L(x_1,x_2,\\lambda).$$\n",
    "The first order optimality conditions lead to\n",
    "$$2x_1+\\lambda=0, \\qquad 2x_2 =0,$$\n",
    "and the Lagrangian is minimized at the point $(x_1,x_2)=(-\\frac{\\lambda}{2},0)$.\n",
    "Thus, the dual function is $$q(\\lambda)= \\frac{\\lambda^2}{4} -\\frac{\\lambda}{2}\\lambda -\\lambda=\n",
    "-\\frac{\\lambda^2}{4} -\\lambda.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c237f1",
   "metadata": {},
   "source": [
    "- Write and solve the dual problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a4e17b",
   "metadata": {},
   "source": [
    "Since there are no constraints on $\\lambda$, the dual problem is\n",
    "$$\\max_{\\lambda\\in\\mathbb{R}} -\\frac{\\lambda^2}{4} -\\lambda.$$\n",
    "To solve it, we use the first order optimality conditions, which lead\n",
    "to $$-\\frac{\\lambda^*}{2}-1 = 0 \\Leftrightarrow \\lambda^* = -2.$$\n",
    "As the second derivative is negative ($-1/2$), the function is\n",
    "concave, and $\\lambda^*$ is indeed a maximum,\n",
    "with optimal value 1. It happens to be the same as the optimal value\n",
    "of the primal. Note that we have seen that strong duality applies to\n",
    "linear optimization, while this optimization problem is\n",
    "nonlinear. Strong duality does not in general apply to non linear\n",
    "optimization problems. It applies only in some specific cases. For\n",
    "instance, when the constraints are linear and the objective function\n",
    "convex, like in this example."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
